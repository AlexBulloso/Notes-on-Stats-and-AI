{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7950d176",
   "metadata": {},
   "source": [
    "### Ch 5 - Resampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753f7a1",
   "metadata": {},
   "source": [
    "#### 5.0 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e3c95e",
   "metadata": {},
   "source": [
    "- Repeatedly drawing samples from a training set and refitting a model of interest on each sample to get more info about the fitted model\n",
    "- Computationally expensive (fitting same method multiple times) but recent advances allow this to be overcome\n",
    "- Two common methods: cross-validation and bootstrap. \n",
    "- Cross-validation: can be used for test error for model assessment or selection.\n",
    "- Bootstrap: can be used to provide a measure of accuracy\n",
    "\n",
    "\n",
    "- Model assessment: evaluating performance\n",
    "- Model selection: choosing proper level of flexibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e87dc47",
   "metadata": {},
   "source": [
    "#### 5.1 Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a35626",
   "metadata": {},
   "source": [
    "- Low test error = good. Easily calculated if test set is available, but this is usually not the case. Training error can easily be calculated. The two are usually different though, with training often underestimating test.\n",
    "- There are methods to estimate the test error rate (chapter 6). \n",
    "- In this section, we designate a part of the training set as the test set. We consider regression for the first few examples.\n",
    "  \n",
    "**Validation Set Approach** \n",
    "- Randomly diving available set into training set and a validation/holdout set. A model is fit on the training set and the fitted model is used to predict observations on the validation set. The validation error rate (usually as MSE) provides an estimate of the test error rate.\n",
    "- Eg. We can use the validation set approach and test the MSE for different orders of a predictor, as done in the book. \n",
    "- The random divisions of the dataset will result in different mean squared errors, and these different divisions show that there is no consensus as to which order performs best on average (Fig 5.2).\n",
    "- While simple and easy to understand, there are drawbacks: \n",
    "  - Validation estimate of test error rate can be variable\n",
    "  - We only train with a subset of observations, so the validation set error rate can overestimate the test error rate.\n",
    "\n",
    "**Leave-One-Out Cross-Validation**\n",
    "- LOOCV splits the set into two parts but instead of creating two subsets, a single observation $(x_1,y_1)$ instead is used for the validation set and the rest make up the training set. The model is fit on the training, and predicted on the single observation.\n",
    "- The excluded observation was not used in the fitting process so the MSE for this observation, $\\text{MSE}_1$, provides an unbiased estimate for the test error, but it is a poor estimate as it is highly variable since its based off of one observation.\n",
    "- We repeat the process by selecting another observation $(x_2,y_2)$ for the validation set, and the rest for training, and computing the $\\text{MSE}_2$. We extend this to all n observations for $\\text{MSE}_1,\\ldots,\\text{MSE}_n$. The estimate for the test MSE is the average of these:\n",
    "$$\\text{CV}_{(n)}=\\frac{1}{n}\\sum_{i=1}^{n}\\text{MSE}_i$$\n",
    "\n",
    "- Advantages\n",
    "  - Far less bias, so LOOCV does not overestimate the test error rate as much as the validation set approach\n",
    "  - LOOCV will always yield the same results.\n",
    "\n",
    "- The main problem of LOOCV is that it can be resource intensive if n is large as the model is fit n times. \n",
    "- With least squares or polynomial regression, we can reduce the computations down to a single formula:\n",
    "$$\\text{CV}_{(n)}=\\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{y_i-\\hat{y}_i}{1-h_i}\\right)^2$$\n",
    "... where $\\hat{y}_i$ is the ith value from the original least squares fit, and $h_i$ is the leverage. \n",
    "- This magic formula doesn't hold in general, which can potentially still make this method intensive\n",
    "\n",
    "**k-Fold Cross-Validation**\n",
    "- The data is divided into observations of k groups (folds) of approximately equal size. The first fold is validation and the rest are training, with an associated $\\text{MSE}_1$ on the validation set. This is repeated K times, until several MSEs are calculated. Finally, the k-fold CV estimate is computed by averaging:\n",
    "$$\\text{CV}_{(k)} = \\frac{1}{k}\\sum_{i=1}^k\\text{MSE}_i$$ \n",
    "- LOOCV is a special case where k = n.\n",
    "- The main advantage is computation - we have k < n and so have (n-k) less computations. There can also be other non-computational advantages eg. one involving bias-variance tradeoff.\n",
    "- CV curves generally have the correct shape but underestimate the true test MSE. We are sometimes interested in the location of the minimum point of the estimated curve and not the estimated MSE, because this allows us to choose the most optimal model for the lowest test error.\n",
    "\n",
    "**k-Fold CV and the Bias-Variance Trade-Off**\n",
    "- LOOCV would be preferred to k-fold CV for reducing bias as it uses a larger sample size for the training data.\n",
    "- LOOCV, however, has a higher variance than k-fold CV with k < n. LOOCV trains on almost the same dataset, so these outputs are highly positively correlated. k-Fold CV with k < n averages the outputs of k fitted models which are less correlated (smaller overlap of training sets).\n",
    "- The mean of many highly correlated quantities (LOOCV) has a higher variance than the mean of many quantities not correlated (k-Fold CV). \n",
    "- From above, we optimally set k = 5 or 10 as these have been shown empirically to yield test error rates suffering neither from high bias or high variance.\n",
    "- \n",
    "**CV on Classification Problems**\n",
    "- CV also works on qualitative data but instead of MSE, we use the number of misclassified observations. For example, for LOOCV:\n",
    "$$\\text{CV}_{(n)}=\\frac{1}{n}\\sum_{i=1}^{n}\\text{Err}_i$$\n",
    "...with $\\text{Err}_i=I(y_i \\neq \\hat{y}_i)$\n",
    "\n",
    "The book uses an example to demonstrate a point (Figs 5.7-8)\n",
    "- Training error rate generally decreases as model complexity increases (overfitting)\n",
    "- Test error displays a characteristic U-shape (bias-variance trade-off)\n",
    "- 10-fold CV used to estimate test error rate provides a good approximation (though still underestimated), and also provides a good estimate to which flexibility is optimal. \n",
    "- Another example is shown with the KNN classifier (1/K as x-axis) which shows similar results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b02363",
   "metadata": {},
   "source": [
    "#### 5.2 The Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed8139c",
   "metadata": {},
   "source": [
    "- Useful as it can be applied to a range of methods\n",
    "\n",
    "Example:\n",
    "- We want to invest a fraction of our money $\\alpha$ in $X$ and the rest $1-\\alpha$ in $Y$. We want to choose $\\alpha$ to minimise the amount total variance of our investment, $\\text{Var}(\\alpha X + (1-\\alpha)Y )$. The minimum is given by \n",
    "$$\\alpha = \\frac{\\sigma^2_Y-\\sigma_{XY}}{\\sigma_X^2+\\sigma^2_Y-2\\sigma_{XY}}$$\n",
    "- The true values for the variances and covariances are unknown but can be estimated from a dataset with past measurements fo $X$ and $Y$. In the example, a thousand estimates for $\\alpha$ were calculated by running the simulation of 100 paired observations of X and Y 1000 times.\n",
    "- Having done this, they obtained a mean $\\bar{\\alpha}$ with a low SE which was very close to the real value.\n",
    "- In practice, we cannot just generate new samples for real data. The bootstrap approach allows a computer to emulate new sample generation to estimate $\\hat{\\alpha}$ without actually generating new samples\n",
    "\n",
    "**The Bootstrap Method (See Fig 5.11)**\n",
    "- Consider a dataset $Z$ with $n=3$ observations. \n",
    "- Select $n$ observations from the data set to produce a bootstrap dataset $Z^{*1}$ with replacement, so the same observation can occur more than once in the bootstrap dataset. Both X and Y values of all observations are included.\n",
    "- This procedure is repeated a large number, $B$, times to produce $B$ bootstrap data sets $Z^{*1},Z^{*2},\\ldots,Z^{*B}$ with $B$ corresponding estimates $\\hat{\\alpha}^{*1},\\hat{\\alpha}^{*2},\\ldots,\\hat{\\alpha}^{*B}$. \n",
    "- The SE can then be calculated as:\n",
    "$$\\text{SE}_B(\\hat{\\alpha})=\\sqrt{\\frac{1}{B-1}\\sum_{r=1}^{B}\\left(\\hat{\\alpha}^{*r}-\\frac{1}{B}\\sum_{r'=1}^{B}\\hat{\\alpha}^{*r'}\\right)}$$\n",
    "- This method can be applied to real data, and looks similar to the simulated data sets. The bootstrap approach can be used effectively to estimate the variability associated with $\\hat{\\alpha}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d246cdb",
   "metadata": {},
   "source": [
    "#### 5.3 Lab: Cross-Validation and the Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "db2559dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS, summarize, poly)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import (cross_validate, KFold, ShuffleSplit)\n",
    "from sklearn.base import clone\n",
    "from ISLP.models import sklearn_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7880531f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 23.61661706966988\n"
     ]
    }
   ],
   "source": [
    "Auto = load_data(\"Auto\")\n",
    "Auto_train, Auto_valid =train_test_split(Auto, test_size=196, random_state=0)\n",
    "\n",
    "X_train = sm.add_constant(Auto_train[['horsepower']])\n",
    "X_valid = sm.add_constant(Auto_valid[['horsepower']])\n",
    "y_train = Auto_train[\"mpg\"]\n",
    "y_valid = Auto_valid[\"mpg\"]\n",
    "\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results = model.fit()\n",
    "predictions = results.predict(X_valid)\n",
    "\n",
    "print(f\"MSE: {np.mean((y_valid - predictions)**2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "742ade74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 23.61661706966988\n"
     ]
    }
   ],
   "source": [
    "Auto = load_data(\"Auto\")\n",
    "Auto_train, Auto_valid = train_test_split(Auto, test_size=196, random_state=0)\n",
    "\n",
    "predictors = \" + \".join(list(filter(lambda x: x != \"horsepower\", Auto.columns)))\n",
    "results = smf.ols(\"mpg ~ horsepower\", Auto_train).fit().predict(Auto_valid)\n",
    "\n",
    "print(f\"MSE: {np.mean((Auto_valid[\"mpg\"] - results)**2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7579762d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23.61661707 18.76303135 18.79694163]\n"
     ]
    }
   ],
   "source": [
    "def add_poly_features(df, col, degree):\n",
    "    df_poly = df.copy()\n",
    "    for d in range(2,degree+1):\n",
    "        df_poly[f\"{col}^{d}\"]=df_poly[col] ** d\n",
    "    return df_poly\n",
    "\n",
    "def evalMSE(terms,response,train,test):\n",
    "    X_train = sm.add_constant(train[terms])\n",
    "    X_test = sm.add_constant(test[terms])\n",
    "    y_train = train[response]\n",
    "    y_test = test[response]\n",
    "    results = sm.OLS(y_train, X_train).fit()\n",
    "    predictions = results.predict(X_test)\n",
    "    return np.mean((predictions - y_test)**2)\n",
    "\n",
    "MSE = np.zeros(3)\n",
    "for idx, degree in enumerate(range(1,4)):\n",
    "    train_poly = add_poly_features(Auto_train,\"horsepower\", degree)\n",
    "    test_poly = add_poly_features(Auto_valid,\"horsepower\", degree)\n",
    "    terms = [\"horsepower\"] + [f\"horsepower^{d}\" for d in range(2,degree+1)]\n",
    "    MSE[idx] = evalMSE(terms,\"mpg\", train_poly, test_poly)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "557fa19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.75540796 16.94510676 16.97437833]\n"
     ]
    }
   ],
   "source": [
    "Auto_train, Auto_valid =train_test_split(Auto, test_size=196, random_state=3)\n",
    "\n",
    "MSE = np.zeros(3)\n",
    "for idx, degree in enumerate(range(1,4)):\n",
    "    train_poly = add_poly_features(Auto_train,\"horsepower\", degree)\n",
    "    test_poly = add_poly_features(Auto_valid,\"horsepower\", degree)\n",
    "    terms = [\"horsepower\"] + [f\"horsepower^{d}\" for d in range(2,degree+1)]\n",
    "    MSE[idx] = evalMSE(terms,\"mpg\", train_poly, test_poly)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fdf308e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(24.231513517929216)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from ISLP.models import sklearn_sm, ModelSpec as islpMS\n",
    "hp_model = sklearn_sm(sm.OLS, islpMS([\"horsepower\"]))\n",
    "X, y = Auto.drop(columns=[\"mpg\"]), Auto[\"mpg\"]\n",
    "cv_results = cross_validate(hp_model, X, y, cv=Auto.shape[0])\n",
    "cv_err = np.mean(cv_results[\"test_score\"])\n",
    "cv_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "26d83ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(24.231513517929216)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class sklearn_sm_new(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, model_type, formula=None, model_args=None):\n",
    "        self.model_type = model_type\n",
    "        self.formula = formula\n",
    "        self.model_args = {} if model_args is None else model_args\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        data = X.copy()\n",
    "        data[\"_y\"] = y\n",
    "        y_design, X_design = patsy.dmatrices(f\"_y ~ {self.formula}\", data, return_type=\"dataframe\")\n",
    "        self.columns_ = X_design.columns\n",
    "        self.model_ = self.model_type(y_design, X_design, **self.model_args).fit()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_design = patsy.dmatrix(f\"{self.formula}\", X, return_type=\"dataframe\")\n",
    "        return self.model_.predict(X_design)\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        y_pred = self.predict(X)\n",
    "        return -np.mean((y - y_pred) ** 2) \n",
    "\n",
    "hp_model = sklearn_sm_new(sm.OLS, formula=\"horsepower\")\n",
    "X, y = Auto.drop(columns=[\"mpg\"]), Auto[\"mpg\"]\n",
    "cv_results = cross_validate(hp_model, X, y, cv=Auto.shape[0]) # cv = K, here K = n so LOOCV\n",
    "cv_err = -np.mean(cv_results[\"test_score\"])\n",
    "cv_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b82a5d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.23151352, 19.24821312, 19.33498406, 19.42443031, 19.03320903])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ISLP.models import sklearn_sm\n",
    "cv_error = np.zeros(5)\n",
    "H = np.array(Auto[\"horsepower\"])\n",
    "M = sklearn_sm(sm.OLS)\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M, X, y, cv=Auto.shape[0])\n",
    "    cv_error[i] = np.mean(M_CV[\"test_score\"])\n",
    "cv_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c53bbe28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.20766449, 19.18533142, 19.27626666, 19.47848402, 19.13722633])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M, X, y, cv=cv)\n",
    "    cv_error[i] = np.mean(M_CV[\"test_score\"])\n",
    "cv_error\n",
    "\n",
    "# Much faster. Quadratic fit still seems most optimal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d86486ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([23.61661707, 22.96552529, 23.43853845, 21.72781699, 22.79416823,\n",
       "        23.09191932, 23.69196999, 23.90184611, 26.53545818, 26.258467  ]),\n",
       " np.float64(23.802232661034164),\n",
       " np.float64(1.4218450941091847))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = ShuffleSplit(n_splits=10, test_size= 196, random_state=0)\n",
    "results = cross_validate(hp_model, Auto.drop([\"mpg\"], axis=1), Auto[\"mpg\"], cv=validation)\n",
    "results[\"test_score\"], results[\"test_score\"].mean(), results[\"test_score\"].std()\n",
    "\n",
    "## The calculated std is not a valid estimate of the sampling variability as\n",
    "## the training samples overlap, thus introducing correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "13b829e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.57583207459283)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ## Bootstrap\n",
    "\n",
    "Portfolio = load_data(\"Portfolio\")\n",
    "def alpha_func(D, idx):\n",
    "    cov_ = np.cov(D[[\"X\",\"Y\"]].loc[idx],rowvar=False)\n",
    "    return ((cov_[1,1] - cov_[0,1])/(cov_[0,0]+cov_[1,1]-2*cov_[0,1])) # Formula 5.7\n",
    "alpha_func(Portfolio, range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9e0bf73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6074452469619004)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "alpha_func(Portfolio, rng.choice(100,100,replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2d87cf92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.09118176521277699)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def boot_SE(func, D, n=None, B=1000, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    first_, second_ = 0,0\n",
    "    n = n or D.shape[0]\n",
    "    for _ in range(B):\n",
    "        idx = rng.choice(D.index, n, replace=True)\n",
    "        value = func(D, idx)\n",
    "        first_ += value\n",
    "        second_ += value**2\n",
    "    return np.sqrt(second_ / B - (first_ / B)**2)\n",
    "\n",
    "alpha_SE = boot_SE(alpha_func, Portfolio, B=1000, seed=0)\n",
    "alpha_SE # Our SE, by the formula, is 0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "102103e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.848807\n",
       "horsepower    0.007352\n",
       "dtype: float64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ## Bootstrapping an OLS model\n",
    "from functools import partial\n",
    "\n",
    "Auto = Auto.reset_index()\n",
    "\n",
    "\n",
    "def boot_OLS(model_matrix, response, D, idx):\n",
    "    D_ = D.loc[idx]\n",
    "    Y_ = D_[response]\n",
    "    X_ = clone(model_matrix).fit_transform(D_)\n",
    "    return sm.OLS(Y_,X_).fit().params\n",
    "\n",
    "hp_func = partial(boot_OLS, MS([\"horsepower\"]), \"mpg\")\n",
    "## func2 = partial(func, par1, par2...)\n",
    "## Partial takes parameters and \"freezes them into func\"\n",
    "## This means func2 is a version of func with a reduced number \n",
    "## of parameters. So, in this case, hp_func = boot_OLS(D, idx)\n",
    "## which is suitable for boot_SE.\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "np.array([hp_func(Auto, rng.choice(392, 392, replace = True)) for _ in range(10)])\n",
    "hp_se = boot_SE(hp_func, Auto, B=1000, seed=10)\n",
    "hp_se\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f004af9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.717499\n",
       "horsepower    0.006446\n",
       "dtype: float64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_model.fit(Auto, Auto[\"mpg\"])\n",
    "hp_model.results_.bse\n",
    "\n",
    "## Using the formula 3.8 under 3.1.2, we obtain SEs of 0.717499 and 0.006446\n",
    "## The standard formulae rely on certain assumptions\n",
    "##      Eg. they depend on the unknown parameter sigma squared, the noise variance\n",
    "##      Sigma squared was then estimated using the RSS\n",
    "##      The formula of the SEs do not rely on the linear model being correct,\n",
    "##      but the estimate for sigma squared does.\n",
    "##      Eg. Assumes that the x_i are fixed and all variability comes from\n",
    "##      variation in the errors epsilon_i\n",
    "## Bootstrap does not rely on these assumptions, and so gives a more accurate\n",
    "## Estimate of the SEs of hat betas 0 and 1 than those from sm.OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3862577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_model = MS([poly(\"horsepower\",2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "496f5273",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'chevrolet citation'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_13112\\2860141283.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    158\u001b[39m rng.choice(392,\n\u001b[32m    159\u001b[39m \u001b[32m392\u001b[39m,\n\u001b[32m    160\u001b[39m replace=\u001b[38;5;28;01mTrue\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;28;01min\u001b[39;00m range(\u001b[32m10\u001b[39m)])\n\u001b[32m    161\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m hp_se = boot_SE(hp_func,\n\u001b[32m    163\u001b[39m Auto,\n\u001b[32m    164\u001b[39m B=\u001b[32m1000\u001b[39m,\n\u001b[32m    165\u001b[39m seed=\u001b[32m10\u001b[39m)\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_13112\\2860141283.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(func, D, n, B, seed)\u001b[39m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;28;01min\u001b[39;00m range(B):\n\u001b[32m    134\u001b[39m         idx = rng.choice(D.index,\n\u001b[32m    135\u001b[39m         n,\n\u001b[32m    136\u001b[39m         replace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         value = func(D, idx)\n\u001b[32m    138\u001b[39m         first_ += value\n\u001b[32m    139\u001b[39m         second_ += value**\u001b[32m2\u001b[39m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.sqrt(second_ / B - (first_ / B)**\u001b[32m2\u001b[39m)\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_13112\\2860141283.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(model_matrix, response, D, idx)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m boot_OLS(model_matrix, response, D, idx):\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     D_ = D.iloc[idx]\n\u001b[32m    150\u001b[39m     Y_ = D_[response]\n\u001b[32m    151\u001b[39m     X_ = clone(model_matrix).fit_transform(D_)\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sm.OLS(Y_, X_).fit().params\n",
      "\u001b[32mc:\\Users\\abull\\Documents\\GitHub\\Notes-on-Stats-and-AI\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1187\u001b[39m             axis = self.axis \u001b[38;5;28;01mor\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m   1188\u001b[39m \n\u001b[32m   1189\u001b[39m             maybe_callable = com.apply_if_callable(key, self.obj)\n\u001b[32m   1190\u001b[39m             maybe_callable = self._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._getitem_axis(maybe_callable, axis=axis)\n",
      "\u001b[32mc:\\Users\\abull\\Documents\\GitHub\\Notes-on-Stats-and-AI\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1739\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._getbool_axis(key, axis=axis)\n\u001b[32m   1740\u001b[39m \n\u001b[32m   1741\u001b[39m         \u001b[38;5;66;03m# a list of integers\u001b[39;00m\n\u001b[32m   1742\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._get_list_axis(key, axis=axis)\n\u001b[32m   1744\u001b[39m \n\u001b[32m   1745\u001b[39m         \u001b[38;5;66;03m# a single integer\u001b[39;00m\n\u001b[32m   1746\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32mc:\\Users\\abull\\Documents\\GitHub\\Notes-on-Stats-and-AI\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1711\u001b[39m         `axis` can only be zero.\n\u001b[32m   1712\u001b[39m         \"\"\"\n\u001b[32m   1713\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1714\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.obj._take_with_is_copy(key, axis=axis)\n\u001b[32m-> \u001b[39m\u001b[32m1715\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m IndexError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1716\u001b[39m             \u001b[38;5;66;03m# re-raise with different error message, e.g. test_getitem_ndarray_3d\u001b[39;00m\n\u001b[32m   1717\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m IndexError(\u001b[33m\"positional indexers are out-of-bounds\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m err\n",
      "\u001b[32mc:\\Users\\abull\\Documents\\GitHub\\Notes-on-Stats-and-AI\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, indices, axis)\u001b[39m\n\u001b[32m   4168\u001b[39m         For Series this does the same \u001b[38;5;28;01mas\u001b[39;00m the public take (it never sets `_is_copy`).\n\u001b[32m   4169\u001b[39m \n\u001b[32m   4170\u001b[39m         See the docstring of `take` \u001b[38;5;28;01mfor\u001b[39;00m full explanation of the parameters.\n\u001b[32m   4171\u001b[39m         \"\"\"\n\u001b[32m-> \u001b[39m\u001b[32m4172\u001b[39m         result = self.take(indices=indices, axis=axis)\n\u001b[32m   4173\u001b[39m         \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[32m   4174\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self.ndim == \u001b[32m2\u001b[39m \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m result._get_axis(axis).equals(self._get_axis(axis)):\n\u001b[32m   4175\u001b[39m             result._set_is_copy(self)\n",
      "\u001b[32mc:\\Users\\abull\\Documents\\GitHub\\Notes-on-Stats-and-AI\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, indices, axis, **kwargs)\u001b[39m\n\u001b[32m   4121\u001b[39m \n\u001b[32m   4122\u001b[39m         nv.validate_take((), kwargs)\n\u001b[32m   4123\u001b[39m \n\u001b[32m   4124\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(indices, slice):\n\u001b[32m-> \u001b[39m\u001b[32m4125\u001b[39m             indices = np.asarray(indices, dtype=np.intp)\n\u001b[32m   4126\u001b[39m             if (\n\u001b[32m   4127\u001b[39m                 axis == \u001b[32m0\u001b[39m\n\u001b[32m   4128\u001b[39m                 \u001b[38;5;28;01mand\u001b[39;00m indices.ndim == \u001b[32m1\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: 'chevrolet citation'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "summarize,\n",
    "poly)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "from sklearn.model_selection import \\\n",
    "(cross_validate,\n",
    "KFold,\n",
    "ShuffleSplit)\n",
    "from sklearn.base import clone\n",
    "from ISLP.models import sklearn_sm\n",
    "\n",
    "Auto = load_data('Auto')\n",
    "Auto_train, Auto_valid = train_test_split(Auto,\n",
    "test_size=196,\n",
    "random_state=0)\n",
    "\n",
    "hp_mm = MS(['horsepower'])\n",
    "X_train = hp_mm.fit_transform(Auto_train)\n",
    "y_train = Auto_train['mpg']\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results = model.fit()\n",
    "\n",
    "X_valid = hp_mm.transform(Auto_valid)\n",
    "y_valid = Auto_valid['mpg']\n",
    "valid_pred = results.predict(X_valid)\n",
    "np.mean((y_valid - valid_pred)**2)\n",
    "\n",
    "def evalMSE(terms, response, train, test):\n",
    "    mm = MS(terms)\n",
    "    X_train = mm.fit_transform(train)\n",
    "    y_train = train[response]\n",
    "    X_test = mm.transform(test)\n",
    "    y_test = test[response]\n",
    "    results = sm.OLS(y_train, X_train).fit()\n",
    "    test_pred = results.predict(X_test)\n",
    "    return np.mean((y_test - test_pred)**2)\n",
    "\n",
    "MSE = np.zeros(3)\n",
    "for idx, degree in enumerate(range(1, 4)):\n",
    "    MSE[idx] = evalMSE([poly('horsepower', degree)],\n",
    "    'mpg',\n",
    "    Auto_train,\n",
    "    Auto_valid)\n",
    "MSE\n",
    "\n",
    "Auto_train, Auto_valid = train_test_split(Auto,\n",
    "test_size=196,\n",
    "random_state=3)\n",
    "MSE = np.zeros(3)\n",
    "for idx, degree in enumerate(range(1, 4)):\n",
    "    MSE[idx] = evalMSE([poly('horsepower', degree)],\n",
    "    'mpg',\n",
    "    Auto_train,\n",
    "    Auto_valid)\n",
    "MSE\n",
    "hp_model = sklearn_sm(sm.OLS,\n",
    "MS(['horsepower']))\n",
    "X, Y = Auto.drop(columns=['mpg']), Auto['mpg']\n",
    "cv_results = cross_validate(hp_model,\n",
    "X,\n",
    "Y,\n",
    "cv=Auto.shape[0])\n",
    "cv_err = np.mean(cv_results['test_score'])\n",
    "cv_err\n",
    "\n",
    "cv_error = np.zeros(5)\n",
    "H = np.array(Auto['horsepower'])\n",
    "M = sklearn_sm(sm.OLS)\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M,\n",
    "    X,\n",
    "    Y,\n",
    "    cv=Auto.shape[0])\n",
    "    cv_error[i] = np.mean(M_CV['test_score'])\n",
    "cv_error\n",
    "\n",
    "cv_error = np.zeros(5)\n",
    "cv = KFold(n_splits=10,\n",
    "shuffle=True,\n",
    "random_state=0) # use same splits for each degree\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M,\n",
    "    X,\n",
    "    Y,\n",
    "    cv=cv)\n",
    "    cv_error[i] = np.mean(M_CV['test_score'])\n",
    "cv_error\n",
    "\n",
    "validation = ShuffleSplit(n_splits=1,\n",
    "test_size=196,\n",
    "random_state=0)\n",
    "results = cross_validate(hp_model,\n",
    "Auto.drop(['mpg'], axis=1),\n",
    "Auto['mpg'],\n",
    "cv=validation);\n",
    "results['test_score']\n",
    "\n",
    "validation = ShuffleSplit(n_splits=10,\n",
    "test_size=196,\n",
    "random_state=0)\n",
    "results = cross_validate(hp_model,\n",
    "Auto.drop(['mpg'], axis=1),\n",
    "Auto['mpg'],\n",
    "cv=validation)\n",
    "results['test_score'].mean(), results['test_score'].std()\n",
    "\n",
    "Portfolio = load_data('Portfolio')\n",
    "def alpha_func(D, idx):\n",
    "    cov_ = np.cov(D[['X','Y']].loc[idx], rowvar=False)\n",
    "    return ((cov_[1,1] - cov_[0,1]) /\n",
    "    (cov_[0,0]+cov_[1,1]-2*cov_[0,1]))\n",
    "alpha_func(Portfolio, range(100))\n",
    "rng = np.random.default_rng(0)\n",
    "alpha_func(Portfolio,\n",
    "rng.choice(100,\n",
    "100,\n",
    "replace=True))\n",
    "\n",
    "def boot_SE(func,\n",
    "D,\n",
    "n=None,\n",
    "B=1000,\n",
    "seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    first_, second_ = 0, 0\n",
    "    n = n or D.shape[0]\n",
    "    for _ in range(B):\n",
    "        idx = rng.choice(D.index,\n",
    "        n,\n",
    "        replace=True)\n",
    "        value = func(D, idx)\n",
    "        first_ += value\n",
    "        second_ += value**2\n",
    "    return np.sqrt(second_ / B - (first_ / B)**2)\n",
    "\n",
    "alpha_SE = boot_SE(alpha_func,\n",
    "Portfolio,\n",
    "B=1000,\n",
    "seed=0)\n",
    "alpha_SE\n",
    "\n",
    "def boot_OLS(model_matrix, response, D, idx):\n",
    "    D_ = D.iloc[idx]\n",
    "    Y_ = D_[response]\n",
    "    X_ = clone(model_matrix).fit_transform(D_)\n",
    "    return sm.OLS(Y_, X_).fit().params\n",
    "\n",
    "hp_func = partial(boot_OLS, MS(['horsepower']), 'mpg')\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "np.array([hp_func(Auto,\n",
    "rng.choice(392,\n",
    "392,\n",
    "replace=True)) for _ in range(10)])\n",
    "\n",
    "hp_se = boot_SE(hp_func,\n",
    "Auto,\n",
    "B=1000,\n",
    "seed=10)\n",
    "hp_se"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
