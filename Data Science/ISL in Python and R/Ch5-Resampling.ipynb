{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7950d176",
   "metadata": {},
   "source": [
    "### Ch 5 - Resampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753f7a1",
   "metadata": {},
   "source": [
    "#### 5.0 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e3c95e",
   "metadata": {},
   "source": [
    "- Repeatedly drawing samples from a training set and refitting a model of interest on each sample to get more info about the fitted model\n",
    "- Computationally expensive (fitting same method multiple times) but recent advances allow this to be overcome\n",
    "- Two common methods: cross-validation and bootstrap. \n",
    "- Cross-validation: can be used for test error for model assessment or selection.\n",
    "- Bootstrap: can be used to provide a measure of accuracy\n",
    "\n",
    "\n",
    "- Model assessment: evaluating performance\n",
    "- Model selection: choosing proper level of flexibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e87dc47",
   "metadata": {},
   "source": [
    "#### 5.1 Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a35626",
   "metadata": {},
   "source": [
    "- Low test error = good. Easily calculated if test set is available, but this is usually not the case. Training error can easily be calculated. The two are usually different though, with training often underestimating test.\n",
    "- There are methods to estimate the test error rate (chapter 6). \n",
    "- In this section, we designate a part of the training set as the test set. We consider regression for the first few examples.\n",
    "  \n",
    "**Validation Set Approach** \n",
    "- Randomly dividing available set into training set and a validation/holdout set. A model is fit on the training set and the fitted model is used to predict observations on the validation set. The validation error rate (usually as MSE) provides an estimate of the test error rate.\n",
    "- Eg. We can use the validation set approach and test the MSE for different orders of a predictor, as done in the book. \n",
    "- The random divisions of the dataset will result in different mean squared errors, and these different divisions show that there is no consensus as to which order performs best on average (Fig 5.2).\n",
    "- While simple and easy to understand, there are drawbacks: \n",
    "  - Validation estimate of test error rate can be variable\n",
    "  - We only train with a subset of observations, so the validation set error rate can overestimate the test error rate.\n",
    "\n",
    "**Leave-One-Out Cross-Validation**\n",
    "- LOOCV splits the set into two parts but instead of creating two subsets, a single observation $(x_1,y_1)$ instead is used for the validation set and the rest make up the training set. The model is fit on the training, and predicted on the single observation.\n",
    "- The excluded observation was not used in the fitting process so the MSE for this observation, $\\text{MSE}_1$, provides an unbiased estimate for the test error, but it is a poor estimate as it is highly variable since its based off of one observation.\n",
    "- We repeat the process by selecting another observation $(x_2,y_2)$ for the validation set, and the rest for training, and computing the $\\text{MSE}_2$. We extend this to all n observations for $\\text{MSE}_1,\\ldots,\\text{MSE}_n$. The estimate for the test MSE is the average of these:\n",
    "  \n",
    "$$ \\text{CV}_{(n)}=\\frac{1}{n}\\sum_{i=1}^{n}\\text{MSE}_{i} $$\n",
    "\n",
    "- Advantages\n",
    "  - Far less bias, so LOOCV does not overestimate the test error rate as much as the validation set approach\n",
    "  - LOOCV will always yield the same results.\n",
    "\n",
    "- The main problem of LOOCV is that it can be resource intensive if n is large as the model is fit n times. \n",
    "- With least squares or polynomial regression, we can reduce the computations down to a single formula:\n",
    "$$\\text{CV}_{(n)}=\\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{y_i-\\hat{y}_i}{1-h_i}\\right)^2$$\n",
    "... where $\\hat{y}_i$ is the ith value from the original least squares fit, and $h_i$ is the leverage. \n",
    "- This magic formula doesn't hold in general, which can potentially still make this method intensive\n",
    "\n",
    "**k-Fold Cross-Validation**\n",
    "- The data is divided into observations of k groups (folds) of approximately equal size. The first fold is validation and the rest are training, with an associated $\\text{MSE}_1$ on the validation set. This is repeated K times, until several MSEs are calculated. Finally, the k-fold CV estimate is computed by averaging:\n",
    "\n",
    "$$\\text{CV}_{(k)} = \\frac{1}{k}\\sum_{i=1}^{k}\\text{MSE}_{i}$$\n",
    "\n",
    "\n",
    "- LOOCV is a special case where k = n.\n",
    "- The main advantage is computation - we have k < n and so have (n-k) less computations. There can also be other non-computational advantages eg. one involving bias-variance tradeoff.\n",
    "- CV curves generally have the correct shape but underestimate the true test MSE. We are sometimes interested in the location of the minimum point of the estimated curve and not the estimated MSE, because this allows us to choose the most optimal model for the lowest test error.\n",
    "\n",
    "**k-Fold CV and the Bias-Variance Trade-Off**\n",
    "- LOOCV would be preferred to k-fold CV for reducing bias as it uses a larger sample size for the training data.\n",
    "- LOOCV, however, has a higher variance than k-fold CV with k < n. LOOCV trains on almost the same dataset, so these outputs are highly positively correlated. k-Fold CV with k < n averages the outputs of k fitted models which are less correlated (smaller overlap of training sets).\n",
    "- The mean of many highly correlated quantities (LOOCV) has a higher variance than the mean of many quantities not correlated (k-Fold CV). \n",
    "- From above, we optimally set k = 5 or 10 as these have been shown empirically to yield test error rates suffering neither from high bias or high variance.\n",
    "\n",
    "\n",
    "**CV on Classification Problems**\n",
    "- CV also works on qualitative data but instead of MSE, we use the number of misclassified observations. For example, for LOOCV:\n",
    "\n",
    "\n",
    "$$ \\text{CV}_{(n)}=\\frac{1}{n}\\sum_{i=1}^{n}\\text{Err}_{i} $$\n",
    "\n",
    "...with $\\text{Err}_i=I(y_i \\neq \\hat{y}_i)$\n",
    "\n",
    "The book uses an example to demonstrate a point (Figs 5.7-8)\n",
    "- Training error rate generally decreases as model complexity increases (overfitting)\n",
    "- Test error displays a characteristic U-shape (bias-variance trade-off)\n",
    "- 10-fold CV used to estimate test error rate provides a good approximation (though still underestimated), and also provides a good estimate to which flexibility is optimal. \n",
    "- Another example is shown with the KNN classifier (1/K as x-axis) which shows similar results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b02363",
   "metadata": {},
   "source": [
    "#### 5.2 The Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed8139c",
   "metadata": {},
   "source": [
    "- Useful as it can be applied to a range of methods\n",
    "\n",
    "Example:\n",
    "- We want to invest a fraction of our money $\\alpha$ in $X$ and the rest $1-\\alpha$ in $Y$. We want to choose $\\alpha$ to minimise the amount total variance of our investment, $\\text{Var}(\\alpha X + (1-\\alpha)Y )$. The minimum is given by \n",
    "$$\\alpha = \\frac{\\sigma^2_Y-\\sigma_{XY}}{\\sigma_X^2+\\sigma^2_Y-2\\sigma_{XY}}$$\n",
    "- The true values for the variances and covariances are unknown but can be estimated from a dataset with past measurements fo $X$ and $Y$. In the example, a thousand estimates for $\\alpha$ were calculated by running the simulation of 100 paired observations of X and Y 1000 times.\n",
    "- Having done this, they obtained a mean $\\bar{\\alpha}$ with a low SE which was very close to the real value.\n",
    "- In practice, we cannot just generate new samples for real data. The bootstrap approach allows a computer to emulate new sample generation to estimate $\\hat{\\alpha}$ without actually generating new samples\n",
    "\n",
    "**The Bootstrap Method (See Fig 5.11)**\n",
    "- Consider a dataset $Z$ with $n=3$ observations. \n",
    "- Select $n$ observations from the data set to produce a bootstrap dataset $Z^{*1}$ with replacement, so the same observation can occur more than once in the bootstrap dataset. Both X and Y values of all observations are included.\n",
    "- This procedure is repeated a large number, $B$, times to produce $B$ bootstrap data sets $Z^{*1},Z^{*2},\\ldots,Z^{*B}$ with $B$ corresponding estimates $\\hat{\\alpha}^{*1},\\hat{\\alpha}^{*2},\\ldots,\\hat{\\alpha}^{*B}$. \n",
    "- The SE can then be calculated as:\n",
    "$$\\text{SE}_B(\\hat{\\alpha})=\\sqrt{\\frac{1}{B-1}\\sum_{r=1}^{B}\\left(\\hat{\\alpha}^{*r}-\\frac{1}{B}\\sum_{r'=1}^{B}\\hat{\\alpha}^{*r'}\\right)}$$\n",
    "- This method can be applied to real data, and looks similar to the simulated data sets. The bootstrap approach can be used effectively to estimate the variability associated with $\\hat{\\alpha}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d246cdb",
   "metadata": {},
   "source": [
    "#### 5.3 Lab: Cross-Validation and the Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db2559dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS, summarize, poly)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import (cross_validate, KFold, ShuffleSplit)\n",
    "from sklearn.base import clone\n",
    "from ISLP.models import sklearn_sm\n",
    "\n",
    "## The problem with ISLP is that it uses the ISLP\n",
    "## library which isn't used in conventional data science.\n",
    "## I would prefer if we implemented code from standard libraries\n",
    "## Like patsy and MS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7880531f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 23.61661706966988\n"
     ]
    }
   ],
   "source": [
    "Auto = load_data(\"Auto\")\n",
    "Auto_train, Auto_valid =train_test_split(Auto, test_size=196, random_state=0)\n",
    "\n",
    "X_train = sm.add_constant(Auto_train[['horsepower']])\n",
    "X_valid = sm.add_constant(Auto_valid[['horsepower']])\n",
    "y_train = Auto_train[\"mpg\"]\n",
    "y_valid = Auto_valid[\"mpg\"]\n",
    "\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results = model.fit()\n",
    "predictions = results.predict(X_valid)\n",
    "\n",
    "print(f\"MSE: {np.mean((y_valid - predictions)**2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742ade74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 23.61661706966988\n"
     ]
    }
   ],
   "source": [
    "Auto = load_data(\"Auto\")\n",
    "Auto_train, Auto_valid = train_test_split(Auto, test_size=196, random_state=0)\n",
    "\n",
    "predictors = \" + \".join(list(filter(lambda x: x != \"horsepower\", Auto.columns)))\n",
    "results = smf.ols(\"mpg ~ horsepower\", Auto_train).fit().predict(Auto_valid)\n",
    "\n",
    "print(f\"MSE: {np.mean((Auto_valid[\"mpg\"] - results)**2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7579762d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23.61661707 18.76303135 18.79694163]\n"
     ]
    }
   ],
   "source": [
    "def add_poly_features(df, col, degree):\n",
    "    df_poly = df.copy()\n",
    "    for d in range(2,degree+1):\n",
    "        df_poly[f\"{col}^{d}\"]=df_poly[col] ** d\n",
    "    return df_poly\n",
    "\n",
    "def evalMSE(terms,response,train,test):\n",
    "    X_train = sm.add_constant(train[terms])\n",
    "    X_test = sm.add_constant(test[terms])\n",
    "    y_train = train[response]\n",
    "    y_test = test[response]\n",
    "    results = sm.OLS(y_train, X_train).fit()\n",
    "    predictions = results.predict(X_test)\n",
    "    return np.mean((predictions - y_test)**2)\n",
    "\n",
    "MSE = np.zeros(3)\n",
    "for idx, degree in enumerate(range(1,4)):\n",
    "    train_poly = add_poly_features(Auto_train,\"horsepower\", degree)\n",
    "    test_poly = add_poly_features(Auto_valid,\"horsepower\", degree)\n",
    "    terms = [\"horsepower\"] + [f\"horsepower^{d}\" for d in range(2,degree+1)]\n",
    "    MSE[idx] = evalMSE(terms,\"mpg\", train_poly, test_poly)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "557fa19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.75540796 16.94510676 16.97437833]\n"
     ]
    }
   ],
   "source": [
    "Auto_train, Auto_valid =train_test_split(Auto, test_size=196, random_state=3)\n",
    "\n",
    "MSE = np.zeros(3)\n",
    "for idx, degree in enumerate(range(1,4)):\n",
    "    train_poly = add_poly_features(Auto_train,\"horsepower\", degree)\n",
    "    test_poly = add_poly_features(Auto_valid,\"horsepower\", degree)\n",
    "    terms = [\"horsepower\"] + [f\"horsepower^{d}\" for d in range(2,degree+1)]\n",
    "    MSE[idx] = evalMSE(terms,\"mpg\", train_poly, test_poly)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf308e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(24.231513517929216)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from ISLP.models import sklearn_sm, ModelSpec as islpMS\n",
    "hp_model = sklearn_sm(sm.OLS, islpMS([\"horsepower\"]))\n",
    "X, y = Auto.drop(columns=[\"mpg\"]), Auto[\"mpg\"]\n",
    "cv_results = cross_validate(hp_model, X, y, cv=Auto.shape[0])\n",
    "cv_err = np.mean(cv_results[\"test_score\"])\n",
    "cv_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26d83ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(24.231513517929216)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "\n",
    "## Implementation of sklearn_sm wrapper\n",
    "class sklearn_sm_new(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, model_type, formula=None, model_args=None):\n",
    "        self.model_type = model_type\n",
    "        self.formula = formula\n",
    "        self.model_args = {} if model_args is None else model_args\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        data = X.copy()\n",
    "        data[\"_y\"] = y\n",
    "        y_design, X_design = patsy.dmatrices(f\"_y ~ {self.formula}\", data, return_type=\"dataframe\")\n",
    "        self.columns_ = X_design.columns\n",
    "        self.model_ = self.model_type(y_design, X_design, **self.model_args).fit()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_design = patsy.dmatrix(f\"{self.formula}\", X, return_type=\"dataframe\")\n",
    "        return self.model_.predict(X_design)\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        y_pred = self.predict(X)\n",
    "        return -np.mean((y - y_pred) ** 2) \n",
    "\n",
    "hp_model = sklearn_sm_new(sm.OLS, formula=\"horsepower\")\n",
    "X, y = Auto.drop(columns=[\"mpg\"]), Auto[\"mpg\"]\n",
    "cv_results = cross_validate(hp_model, X, y, cv=Auto.shape[0]) # cv = K, here K = n so LOOCV\n",
    "cv_err = -np.mean(cv_results[\"test_score\"])\n",
    "cv_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5443b48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(24.231513517929216)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from ISLP.models import sklearn_sm, ModelSpec as islpMS\n",
    "hp_model = sklearn_sm(sm.OLS, islpMS([\"horsepower\"]))\n",
    "X, y = Auto.drop(columns=[\"mpg\"]), Auto[\"mpg\"]\n",
    "cv_results = cross_validate(hp_model, X, y, cv=Auto.shape[0])\n",
    "cv_err = np.mean(cv_results[\"test_score\"])\n",
    "cv_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b82a5d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.23151352, 19.24821312, 19.33498406, 19.42443031, 19.03320903])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ISLP.models import sklearn_sm\n",
    "cv_error = np.zeros(5)\n",
    "H = np.array(Auto[\"horsepower\"])\n",
    "M = sklearn_sm(sm.OLS)\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M, X, y, cv=Auto.shape[0])\n",
    "    cv_error[i] = np.mean(M_CV[\"test_score\"])\n",
    "cv_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c53bbe28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.20766449, 19.18533142, 19.27626666, 19.47848402, 19.13722633])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M, X, y, cv=cv)\n",
    "    cv_error[i] = np.mean(M_CV[\"test_score\"])\n",
    "cv_error\n",
    "\n",
    "# Much faster. Quadratic fit still seems most optimal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d86486ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([23.61661707, 22.96552529, 23.43853845, 21.72781699, 22.79416823,\n",
       "        23.09191932, 23.69196999, 23.90184611, 26.53545818, 26.258467  ]),\n",
       " np.float64(23.802232661034164),\n",
       " np.float64(1.4218450941091847))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = ShuffleSplit(n_splits=10, test_size= 196, random_state=0)\n",
    "results = cross_validate(hp_model, Auto.drop([\"mpg\"], axis=1), Auto[\"mpg\"], cv=validation)\n",
    "results[\"test_score\"], results[\"test_score\"].mean(), results[\"test_score\"].std()\n",
    "\n",
    "## The calculated std is not a valid estimate of the sampling variability as\n",
    "## the training samples overlap, thus introducing correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13b829e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.57583207459283)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ## Bootstrap\n",
    "\n",
    "Portfolio = load_data(\"Portfolio\")\n",
    "def alpha_func(D, idx):\n",
    "    cov_ = np.cov(D[[\"X\",\"Y\"]].loc[idx],rowvar=False)\n",
    "    return ((cov_[1,1] - cov_[0,1])/(cov_[0,0]+cov_[1,1]-2*cov_[0,1])) # Formula 5.7\n",
    "alpha_func(Portfolio, range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e0bf73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6074452469619004)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "alpha_func(Portfolio, rng.choice(100,100,replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d87cf92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.09118176521277699)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def boot_SE(func, D, n=None, B=1000, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    first_, second_ = 0,0\n",
    "    n = n or D.shape[0]\n",
    "    for _ in range(B):\n",
    "        idx = rng.choice(D.index, n, replace=True)\n",
    "        value = func(D, idx)\n",
    "        first_ += value\n",
    "        second_ += value**2\n",
    "    return np.sqrt(second_ / B - (first_ / B)**2)\n",
    "\n",
    "alpha_SE = boot_SE(alpha_func, Portfolio, B=1000, seed=0)\n",
    "alpha_SE # Our SE, by the formula, is 0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "102103e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.848807\n",
       "horsepower    0.007352\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ## Bootstrapping an OLS model\n",
    "from functools import partial\n",
    "\n",
    "Auto = Auto.reset_index()\n",
    "\n",
    "\n",
    "def boot_OLS(model_matrix, response, D, idx):\n",
    "    D_ = D.loc[idx]\n",
    "    Y_ = D_[response]\n",
    "    X_ = clone(model_matrix).fit_transform(D_)\n",
    "    return sm.OLS(Y_,X_).fit().params\n",
    "\n",
    "hp_func = partial(boot_OLS, MS([\"horsepower\"]), \"mpg\")\n",
    "## func2 = partial(func, par1, par2...)\n",
    "## Partial takes parameters and \"freezes them into func\"\n",
    "## This means func2 is a version of func with a reduced number \n",
    "## of parameters. So, in this case, hp_func = boot_OLS(D, idx)\n",
    "## which is suitable for boot_SE.\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "np.array([hp_func(Auto, rng.choice(392, 392, replace = True)) for _ in range(10)])\n",
    "hp_se = boot_SE(hp_func, Auto, B=1000, seed=10)\n",
    "hp_se\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f004af9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.717499\n",
       "horsepower    0.006446\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_model.fit(Auto, Auto[\"mpg\"])\n",
    "hp_model.results_.bse\n",
    "\n",
    "## Using the formula 3.8 under 3.1.2, we obtain SEs of 0.717499 and 0.006446\n",
    "## The standard formulae rely on certain assumptions\n",
    "##      Eg. they depend on the unknown parameter sigma squared, the noise variance\n",
    "##      Sigma squared was then estimated using the RSS\n",
    "##      The formula of the SEs do not rely on the linear model being correct,\n",
    "##      but the estimate for sigma squared does.\n",
    "##      Eg. Assumes that the x_i are fixed and all variability comes from\n",
    "##      variation in the errors epsilon_i\n",
    "## Bootstrap does not rely on these assumptions, and so gives a more accurate\n",
    "## Estimate of the SEs of hat betas 0 and 1 than those from sm.OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3862577e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept                                  2.067840\n",
       "poly(horsepower, degree=2, raw=True)[0]    0.033019\n",
       "poly(horsepower, degree=2, raw=True)[1]    0.000120\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quad_model = MS([poly(\"horsepower\",2, raw=True)])\n",
    "quad_func = partial(boot_OLS, quad_model, \"mpg\")\n",
    "boot_SE(quad_func, Auto, B=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb2ff71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept                                  1.800\n",
       "poly(horsepower, degree=2, raw=True)[0]    0.031\n",
       "poly(horsepower, degree=2, raw=True)[1]    0.000\n",
       "Name: std err, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = sm.OLS(Auto[\"mpg\"], quad_model.fit_transform(Auto))\n",
    "summarize(M.fit())[\"std err\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46737d76",
   "metadata": {},
   "source": [
    "#### 5.4a Conceptual Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc59db7",
   "metadata": {},
   "source": [
    "1. Need to use identity $V(aX + bY) = a^2V(X) + b^2V(Y) + 2ab\\text{Cov}(X,Y)$ and differentiate, with $\\frac{\\text{d}}{\\text{d}a}=0$\n",
    "2. a. (n-1)/n as there is a 1/n chance of selecting it\n",
    "   \n",
    "   b. (n-1)/n as the jth observation is not removed\n",
    "\n",
    "   c. There is an equally likely chance (n-1)/n that the jth element is not removed, and there are n elements, so the chance j is not included is (1-1/n)^n\n",
    "\n",
    "   d. When n = 5, the probability it's in the sample is 1 - (1-1/5)^5 = 0.67232\n",
    "\n",
    "   e. When n = 100, the probability it's in the sample is 1 - (1-1/100)^100 = 0.6334\n",
    "\n",
    "   f. When n = 10000, the probability it's in the sample is 1 - (1-1/10000)^10000 = 0.6321\n",
    "\n",
    "   g + h (see below)\n",
    "\n",
    "3. a. K Fold Cross validation is implemented by first splitting the dataset into K folds of roughly equal amounts of observations. One fold is chosen to be excluded, and the rest of the folds are used for fitting a model. The excluded fold is used as the validation dataset. The process is repeated for all K folds. The CV estimate is the average of all the MSEs calculated.\n",
    "   \n",
    "   b.\n",
    "\n",
    "      i. Adv: K Fold does not give as variable test error estimates, validation set does (lower variance) due to using all observations in training and validation.\n",
    "      Disadv: Validation set is simpler and easier to understand, and less computationally intensive. \n",
    "      \n",
    "      ii. Adv: K Fold is less computationally intensive than LOOCV. K Fold also has lower variance as training sets differ between folds. Disadv: In general, LOOCV can provide more accurate estimates due to using larger training sets. LOOCV also has lower bias.\n",
    "4. We can use a bootstrap method. This relies on repeatedly sampling B samples of size n from the original dataset with replacement and refitting a model, predicting the same X. A standard deviation can then be calculated from the estimates of Y calculated from the bootstrap datasets, which gives an approximate for the standard deviation of Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2a4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6321223982317534\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 2. g\n",
    "from matplotlib.pyplot import subplots\n",
    "fig, ax = subplots(figsize=(8,8))\n",
    "x = np.arange(1,100000)\n",
    "y = 1 - (1 - 1/x)**x\n",
    "ax.scatter(x,y, marker=\".\")\n",
    "ax.set_yscale(\"linear\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.axhline(1-(1-1/100000)**100000, linestyle=\"--\")\n",
    "print(1-(1-1/100000)**100000);\n",
    "## Value tends to (1 - 1/e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e578f850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0089)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2. h\n",
    "\n",
    "rng = np.random.default_rng(10)\n",
    "store = np.empty(10000)\n",
    "for i in range(10000):\n",
    "    store[i] = np.sum(rng.choice(100, replace=True) == 4) > 0\n",
    "np.mean(store)\n",
    "\n",
    "## In only 0.9% of the bootstrap samples, the observation is not included\n",
    "## So the samples should generally be representative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd384b9",
   "metadata": {},
   "source": [
    "#### 5.4b Applied Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "70f78397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>default</td>     <th>  No. Observations:  </th>  <td> 10000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>  9997</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>         <td>Logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -789.48</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 14 Aug 2025</td> <th>  Deviance:          </th> <td>  1579.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>23:42:05</td>     <th>  Pearson chi2:      </th> <td>6.95e+03</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>9</td>        <th>  Pseudo R-squ. (CS):</th>  <td>0.1256</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>  -11.5405</td> <td>    0.435</td> <td>  -26.544</td> <td> 0.000</td> <td>  -12.393</td> <td>  -10.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income</th>  <td> 2.081e-05</td> <td> 4.99e-06</td> <td>    4.174</td> <td> 0.000</td> <td>  1.1e-05</td> <td> 3.06e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>balance</th> <td>    0.0056</td> <td>    0.000</td> <td>   24.835</td> <td> 0.000</td> <td>    0.005</td> <td>    0.006</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}   &     default      & \\textbf{  No. Observations:  } &    10000    \\\\\n",
       "\\textbf{Model:}           &       GLM        & \\textbf{  Df Residuals:      } &     9997    \\\\\n",
       "\\textbf{Model Family:}    &     Binomial     & \\textbf{  Df Model:          } &        2    \\\\\n",
       "\\textbf{Link Function:}   &      Logit       & \\textbf{  Scale:             } &    1.0000   \\\\\n",
       "\\textbf{Method:}          &       IRLS       & \\textbf{  Log-Likelihood:    } &   -789.48   \\\\\n",
       "\\textbf{Date:}            & Thu, 14 Aug 2025 & \\textbf{  Deviance:          } &    1579.0   \\\\\n",
       "\\textbf{Time:}            &     23:42:05     & \\textbf{  Pearson chi2:      } &  6.95e+03   \\\\\n",
       "\\textbf{No. Iterations:}  &        9         & \\textbf{  Pseudo R-squ. (CS):} &   0.1256    \\\\\n",
       "\\textbf{Covariance Type:} &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                 & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}   &     -11.5405  &        0.435     &   -26.544  &         0.000        &      -12.393    &      -10.688     \\\\\n",
       "\\textbf{income}  &    2.081e-05  &     4.99e-06     &     4.174  &         0.000        &      1.1e-05    &     3.06e-05     \\\\\n",
       "\\textbf{balance} &       0.0056  &        0.000     &    24.835  &         0.000        &        0.005    &        0.006     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Generalized Linear Model Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                default   No. Observations:                10000\n",
       "Model:                            GLM   Df Residuals:                     9997\n",
       "Model Family:                Binomial   Df Model:                            2\n",
       "Link Function:                  Logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -789.48\n",
       "Date:                Thu, 14 Aug 2025   Deviance:                       1579.0\n",
       "Time:                        23:42:05   Pearson chi2:                 6.95e+03\n",
       "No. Iterations:                     9   Pseudo R-squ. (CS):             0.1256\n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        -11.5405      0.435    -26.544      0.000     -12.393     -10.688\n",
       "income      2.081e-05   4.99e-06      4.174      0.000     1.1e-05    3.06e-05\n",
       "balance        0.0056      0.000     24.835      0.000       0.005       0.006\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 5. a\n",
    "\n",
    "from ISLP import load_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_data(\"default\")\n",
    "x = pd.DataFrame(data[[\"income\", \"balance\"]])\n",
    "y = data[\"default\"].map(lambda x: 1 if x == \"Yes\" else 0)\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "model = sm.GLM(y, x, family = sm.families.Binomial())\n",
    "model.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16bf15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "## 5. b + c\n",
    "\n",
    "(x_train, x_test, y_train, y_test) = train_test_split(x, y, test_size = 0.2, random_state=1)\n",
    "x_train, x_test, y_train, y_test\n",
    "\n",
    "model2 = sm.GLM(y_train, x_train, family = sm.families.Binomial())\n",
    "results = model2.fit().predict(x_test).map(lambda x: 1 if x > 0.5 else 0)\n",
    "print(np.mean(results != y_test)*100)\n",
    "\n",
    "# Random state = 1:\n",
    "# Test 0.2 => 2.7% error\n",
    "# Test 0.4 => 2.5% error \n",
    "# Test 0.1 => 3.0% error\n",
    "\n",
    "# Varying the splits varies error between 2.0 - 3.0 %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db63c967",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdb4cb62",
   "metadata": {},
   "source": [
    "#### SUMMARY (Alt Lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bcfad67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard Imports\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import (cross_validate, KFold, ShuffleSplit)\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "59b72834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(23.61661706966988)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Validation Set Approach\n",
    "\n",
    "\n",
    "Auto = load_data(\"Auto\")\n",
    "X = Auto[[\"horsepower\"]].values\n",
    "y = Auto[\"mpg\"].values\n",
    "\n",
    "Auto_train, Auto_valid = train_test_split(Auto, test_size=196, random_state=0)\n",
    "X_train = sm.add_constant(Auto_train[['horsepower']])\n",
    "X_valid = sm.add_constant(Auto_valid[['horsepower']])\n",
    "y_train = Auto_train['mpg']\n",
    "y_valid = Auto_valid['mpg']\n",
    "\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "predictions = model.predict(X_valid)\n",
    "error_mse = np.mean((y_valid-predictions)**2)\n",
    "error_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a04f1908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.61661707, 18.76303135, 18.79694163])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Validation Set for Higher Orders\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "def evalMSE(degree, response, train, test):\n",
    "    poly = PolynomialFeatures(degree, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(train[['horsepower']])\n",
    "    X_test_poly = poly.transform(test[['horsepower']])\n",
    "    X_train_sm = sm.add_constant(X_train_poly)\n",
    "    X_test_sm = sm.add_constant(X_test_poly)\n",
    "    y_train = train[response]\n",
    "    y_test = test[response]\n",
    "    model = sm.OLS(y_train, X_train_sm).fit()\n",
    "    predictions = model.predict(X_test_sm)\n",
    "    return np.mean((y_test - predictions) ** 2)\n",
    "\n",
    "MSE = np.zeros(3)\n",
    "for idx, degree in enumerate(range(1,4)):\n",
    "    MSE[idx] = evalMSE(degree,\"mpg\",Auto_train,Auto_valid)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ac2a24df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross-Validation Set-up (wrapper)\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "class sklearn_sm(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.results_ = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Don't add constant here, already included in X\n",
    "        self.results_ = self.model(y, X).fit()\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.results_.predict(X)\n",
    "    \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def generate_poly_features(X, degree):\n",
    "    poly = PolynomialFeatures(degree, include_bias = True)\n",
    "    return poly.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5e293c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.23151352 19.24821312 19.33498406 19.42443031 19.03320903]\n"
     ]
    }
   ],
   "source": [
    "## LOOCV\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_error = np.zeros(5)\n",
    "H = np.array(Auto['horsepower'])\n",
    "X = np.power.outer(H, np.arange(d+1)) \n",
    "Y = np.array(Auto['mpg'])\n",
    "M = sklearn_sm(sm.OLS)\n",
    "\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))  # includes intercept column\n",
    "    M_CV = cross_validate(M, X, Y, cv=Auto.shape[0], scoring=\"neg_mean_squared_error\")\n",
    "    cv_error[i] = -np.mean(M_CV['test_score'])\n",
    "\n",
    "print(cv_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0030ca65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.20766449, 19.18533142, 19.27626666, 19.47848402, 19.13722633])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## KFold\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv_error = np.zeros(5)\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "for i,d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H,np.arange(d+1))\n",
    "    M_CV = cross_validate(M,X,Y,cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "    cv_error[i] = -np.mean(M_CV[\"test_score\"])\n",
    "cv_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae250f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.09118176521277699)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Bootstrap - estimating a statistic of interest\n",
    "\n",
    "Portfolio = load_data('Portfolio')\n",
    "def alpha_func(D, idx):\n",
    "    cov_ = np.cov(D[['X','Y']].loc[idx], rowvar=False)\n",
    "    return ((cov_[1,1] - cov_[0,1])/(cov_[0,0]+cov_[1,1]-2*cov_[0,1])) # standard formula\n",
    "\n",
    "rng = np.random.default_rng(0) # sets seed\n",
    "\n",
    "def boot_SE(func, D, n=None, B=1000, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    first_, second_ = 0, 0\n",
    "    n = n or D.shape[0]\n",
    "    for _ in range(B):\n",
    "        idx = rng.choice(D.index, n, replace=True)\n",
    "        value = func(D, idx) # Returns hat alpha for the randomly chosen datapoint\n",
    "        first_ += value \n",
    "        second_ += value**2\n",
    "    return np.sqrt(second_ / B - (first_ / B)**2)\n",
    "\n",
    "alpha_SE = boot_SE(alpha_func, Portfolio, B=1000, seed=0)\n",
    "alpha_SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "20b697c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(intercept     0.848807\n",
       " horsepower    0.007352\n",
       " dtype: float64,\n",
       " intercept     0.717499\n",
       " horsepower    0.006446\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Bootstrap - estimating a linear regression model\n",
    "Auto = load_data(\"Auto\")\n",
    "Auto = Auto.reset_index()\n",
    "def boot_OLS(model_matrix, response, D, idx):\n",
    "    D_ = D.iloc[idx]\n",
    "    Y_ = D_[response]\n",
    "    X_ = clone(model_matrix).fit_transform(D_)\n",
    "    return sm.OLS(Y_, X_).fit().params\n",
    "\n",
    "hp_func = partial(boot_OLS, MS(['horsepower']), 'mpg')\n",
    "rng = np.random.default_rng(0)\n",
    "np.array([hp_func(Auto,rng.choice(392,392,replace=True)) for _ in range(10)])\n",
    "\n",
    "hp_model.fit(Auto, Auto[\"mpg\"])\n",
    "model_se = hp_model.results_.bse\n",
    "hp_se = boot_SE(hp_func, Auto, B=1000, seed=10)\n",
    "hp_se,model_se "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c18a0657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept                                  2.067840\n",
       "poly(horsepower, degree=2, raw=True)[0]    0.033019\n",
       "poly(horsepower, degree=2, raw=True)[1]    0.000120\n",
       "dtype: float64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Bootstrap - linear regression with \n",
    "\n",
    "quad_model = MS([poly('horsepower', 2, raw=True)])\n",
    "quad_func = partial(boot_OLS, quad_model, 'mpg')\n",
    "boot_SE(quad_func, Auto, B=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "41f15a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept                                  1.800\n",
       "poly(horsepower, degree=2, raw=True)[0]    0.031\n",
       "poly(horsepower, degree=2, raw=True)[1]    0.000\n",
       "Name: std err, dtype: float64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = sm.OLS(Auto['mpg'],\n",
    "quad_model.fit_transform(Auto))\n",
    "summarize(M.fit())['std err']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
